apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: create-cluster-capi-hybrid
  namespace: test-workloads
spec:
  volumes:
  - name: kubeconfig
    secret:
      secretName: standup-kubeconfig
  workspaces:
  - name: cluster
    description: Cluster information is stored here.
  params:
  - name: RELEASE
    type: string
    description: Release ID to use when creating the cluster.
  - name: PROVIDER
    type: string
    default: ""
    description: Name of the provider (either 'aws' or 'azure')
  - name: REGION
    type: string
    default: "default"
    description: Name of the AWS provider (can be 'default', meaning frankfurt, or 'china')
  - name: MC_KUBECONFIG
    type: string
    description: "Path of kubeconfig file to access test management cluster"
  - name: CLUSTER_FLAVOUR
    type: string
    default: "default"
    description: The cluster flavour. Either 'default' or 'eni' (only used in AWS).
  results:
  - name: cluster-id
    description: The ID of the created cluster.
  steps:
  - name: create-cluster
    image: quay.io/giantswarm/kubectl-gs:2.37.0
    env:
      - name: PROVIDER
        value: $(params.PROVIDER)
      - name: REGION
        value: $(params.REGION)
      - name: KUBECONFIG
        value: $(params.MC_KUBECONFIG)
      - name: RELEASE
        value: $(params.RELEASE)
      - name: CLUSTER_FLAVOUR
        value: $(params.CLUSTER_FLAVOUR)
    volumeMounts:
      - name: kubeconfig
        mountPath: /etc/kubeconfig
    script: |
      #! /bin/sh

      set -e

      echo "Provider: '${PROVIDER}'"
      echo "Region: '${REGION}'"
      echo "MC Kubeconfig path: '${KUBECONFIG}'"
      echo "Release: '${RELEASE}'"
      echo "Flavour: '${CLUSTER_FLAVOUR}'"

      echo "WIP"
      exit 1

      # Generate template for control plane.
      kubectl-gs template cluster \
        --provider "${PROVIDER}" \
        --release ${RELEASE} \
        --organization giantswarm \
        --description "e2e test" \
        --output /tmp/cluster.yaml

      # Extract cluster name from template.
      cluster_id=$(cat /tmp/cluster.yaml | grep cluster.x-k8s.io/cluster-name | head -n1 | cut -d" " -f6)

      echo -n "${cluster_id}" >$(workspaces.cluster.path)/cluster-id
      echo -n "${cluster_id}" >$(results.cluster-id.path)

      # AZ is mandatory on AWS, so we need an if here.
      # Also, we run tests for AWS on gaia, in eu-central-1
      AZS=""
      if [ "${PROVIDER}" == "aws" ]
      then
        if [ "${REGION}" == "china" ]
        then
          AZS="--availability-zones cn-north-1a"
        else
          AZS="--availability-zones eu-central-1a"
        fi
      fi

      kubectl-gs template nodepool \
        --provider "${PROVIDER}" \
        --release ${RELEASE} \
        --organization giantswarm \
        --cluster-name "${cluster_id}" \
        --description "np1" \
        --nodes-min 3 \
        --nodes-max 10 \
        $AZS \
        --output /tmp/nodepool.yaml

      set +e
      # This function splits a multi-document yaml file into single yaml manifests
      # and retries applying each manifest up to 5 times in case of failure.
      retry_single() {
        data="$(cat $1 | sed 's/---/ยง/g')"
        IFS="ยง"
        for yml in $data
        do
          if [ "$yml" == "" ] ; then continue; fi
          ok="false"
          echo "Applying: "
          echo "$yml"
          for i in 1 2 3 4 5; do
            echo "$yml" | kubectl --request-timeout=30s apply -f -
            success=$?
            if [ "$success" -eq "0" ]; then
              ok="true"
              break
            fi
          done

          if [ "$ok" != "true" ]
          then
            echo "Failed to apply $1"
            exit 1
          fi
        done
      }

      retry_single /tmp/cluster.yaml
      retry_single /tmp/nodepool.yaml

  - name: legacy-get-kubeconfig
    image: quay.io/giantswarm/kubectl:1.22.3
    env:
      - name: PROVIDER
        value: $(params.PROVIDER)
      - name: REGION
        value: $(params.REGION)
      - name: KUBECONFIG
        value: $(params.MC_KUBECONFIG)
    volumeMounts:
      - name: kubeconfig
        mountPath: /etc/kubeconfig
    script: |
      #!/bin/sh

      echo "Provider: '${PROVIDER}'"
      echo "Region: '${REGION}'"
      echo "MC Kubeconfig path: '${KUBECONFIG}'"

      for i in $(seq 1 20); do
        cluster_id=$(cat $(workspaces.cluster.path)/cluster-id)
        json="$(kubectl -n ${cluster_id} get secret ${cluster_id}-kubeconfig -o json)"
        success=$?
        if [ "$success" -eq "0" ]; then
          echo "${json}"| jq -r .data.kubeConfig | base64 -d >$(workspaces.cluster.path)/kubeconfig

          break
        fi
        sleep 5
      done
      exit $success
